{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette('tab10')\n",
    "import statistics\n",
    "import numpy as np\n",
    "# from geopy import geocoders\n",
    "import pycountry_convert as pc\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da3af2",
   "metadata": {},
   "source": [
    "## Summary (Steps and Findings)\n",
    "\n",
    "### Data Prepocessing\n",
    "\n",
    "We were given the quality grading data of two different coffee species (Arabica and Robusta). To get the data ready for EDA and some modeling, we have applied the following preprocessing steps right in flow.\n",
    " \n",
    "   1.  <b> Combining both dataset:</b> Arabica dataset was much bigger (1300+ instances) than the Robusta one (only 26 records). Therefore, it was better to combine both of the. Through analyzing some statistical metrics (i.e. central tendencies), we saw that both of the datasets are not that different. Hence, we appended the Robusta dataset with Arabica.\n",
    "   \n",
    "   2. <b> Dealing with Missing Values:</b> We notice that there are eight columns with missing values (0.07% to 17%): three columns with continuous (float) values, four categorical columns, and one integer column. We followed a univariate based missing values imputation using the mean/mode of the column within the ('country_of_origin', 'species') group.\n",
    "   \n",
    "   3. <b> Selecting important features:</b> In order to find relevant features, we computed correlation between target variable (quality_score) and other numerical features. We inferred that columns ('quality_score', 'aroma', 'flavor', 'aftertaste', 'acidity', 'body', 'balance', 'clean_cup', 'sweetness', 'cupper_points') are highly correlated with the quality_score, while columns ('moisture', 'category_one_defects', 'category_two_defects') are negatively correlated with our target feature quality_score. We kept both of these sets and dropped other numerical columns\n",
    "   \n",
    "   4. <b> Adding new column: </b> As we are particularly interested to do some analysis on regional bases (i.e. Central America). Therefore, we add a new column as 'continent'. Although, Central America is not a separate continent yet we kept it separate just for our analysis.\n",
    "   \n",
    "   4. <b> Feature Scaling and Encoding: </b> Most of our numerical features were not on a single scale which might hinder the model to converge fast. Moreover, After feature selection, we are also left with some important categorical columns ('species','country_of_origin','variety','processing_method','color'). We applied MinMaxScaler to scale numeric features and OneHotEncoder to encode the categorical features. However, we used this part of preprocessing in the prediction pipeline defined in the Modeling section.\n",
    "   \n",
    "### Exploratory Data Analysis\n",
    "\n",
    " We did some EDA, to find the answers to the questions asked with the challenge,\n",
    "\n",
    "   1. <b>Understand which factors contribute to a high coffee quality score.</b>\n",
    "   \n",
    "   Using the Data Preprocessing and EDA we found the following contributing factors (especially Fig 2, EDA, Fig 3, and regression coefficients)\n",
    "   \n",
    "       1. Positive factors: ('quality_score', 'aroma', 'flavor', 'aftertaste', 'acidity', 'body', 'balance', 'clean_cup', 'sweetness', 'cupper_points') with super high to high correlation. While in categorical variable ('continent', 'country_of_origin','variety') are also highly important.\n",
    "       2. Negative factors: ('moisture', 'category_one_defects', 'category_two_defects') are factors, negatively impact the quality score. Here, 'category_one_defects' and 'category_two_defects' have a medium impact while moisture has a comparatively lower impact.\n",
    "       https://www.coffeestrategies.com/wp-content/uploads/2020/08/Green-Coffee-Defect-Handbook.pdf\n",
    "       \n",
    "   2. <b>Provide a recommendation to a prospective coffee farmer on what to consider to produce a high-quality coffee in Central America.</b>\n",
    "\n",
    "   A major chunk of the records (~44%) are associated with the Central America region. Here, we can infer that the countries in Central America are the biggest producer of coffee. However, we notice on average Central American countries have a lower quality score. We also notice that mostly in the case of highly correlated features, 'Central America' is performing comparatively poor than other continents and better on the negatively correlated features, especially category_two_defect and moisture. Moreover, we notice that the distribution of values for 'Central America' in each feature is quite wide along with having many outliers. Which indicated a sort of inconsistency in production. \n",
    "   \n",
    "   To find the reasons behind this, there could be three factors behind this: 1) variety, 2) any specific country within the region with poor performance, or 3) processing_method. We investigated this draft our suggestions as\n",
    "          \n",
    "        1. The major chunk of production comprises three varieties (Bourban, Typica, and Caturra). We can notice that 'Typica' (holding the second highest chunk) has an inferior score but the highest defect score as compared to the third one 'Catura' and all the varieties in the top 5. So, the first suggestion we can make is to drop Typica or get it least production.\n",
    "        2. Drop 'Typica' or at least decrease its production as compared to the 'Catura' (which is also best in the rest of the regions). 'Catura' can give them a better score with less defect score. Moreover, we have some examples from the coffees produced in small quantities but people liked them very much. May take look at them if it would worthful to promote them more e.g. 'Gesha'.\n",
    "        3. We see that about 72% of the production comes from Mexico (41%) and Guatemala (31%). However, Mexico has a slightly lower score than Guatemala but more importantly very high score for negatively correlated features like moisture, category_one_defect, and category_two_defect. \n",
    "        4. The main reason behind the Mexican farmers getting lower scores is the variety 'Typica'. We already saw that 'Typica' has more defects and higher moisture level than most of the other which make it a bit less attractive.\n",
    "        5. The Farmers in Mexico should more focus on reducing the moisture and reducing defects. This will ultimately get them a better quality score.\n",
    "\n",
    "### Prediction Model\n",
    "\n",
    "To apply or prediction model, we using the following ingredients and steps\n",
    "\n",
    "   1. <b>Data:</b> data prepared in the data preprocessing section We just add the feature scaling encoding function here along with the prediction pipeline.\n",
    "   2. <b>Models: </b> We employed three regression-based model: 1) Linear Regression, 2) Decision Tree Regression, and 3) Random Forest Regression.\n",
    "   3. <b>Evaluate:</b>\n",
    "       1. Cross validate: We use five-fold cross-validation to evaluate our models.\n",
    "       2. Metrics: We use R^2 and Mean Squared Error to evaluate the performance.\n",
    "   4. <b>Results:</b> Among all the three models, linear regression performed better both in terms of R^2 and Mean Squared Error.\n",
    "   \n",
    "<b>Note:</b> I just chose the simple option. However, we can further apply more sophisticated data processing (dealing with outliers or applying power transformations to align data with respect to the normal distribution) or model tuning to get better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gn = geocoders.Google()\n",
    "# gn.geocode(\"Mexico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d3841",
   "metadata": {},
   "source": [
    "We are given with a data of two species 'Arabica' and 'Robusta'. Let's load both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arabica = pd.read_csv('./data/arabica.csv')\n",
    "data_robusta = pd.read_csv('./data/robusta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebfe1ec",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f5f2e",
   "metadata": {},
   "source": [
    "### Combining both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6423a",
   "metadata": {},
   "source": [
    "First let's explore both datasets and see if we can merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4da527",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arabica.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arabica.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a153442",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arabica.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f96555",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_robusta.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_robusta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arabica.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e2ee0",
   "metadata": {},
   "source": [
    "We saw that statistical features of both datasets are not that different. Meanwhile, data_robusta is not that big that we can build that modely solely on that dataset. Let's append(merge at the end) data_robusta with the data_arabica and use that dataset for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e82d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_robusta = data_robusta[data_arabica.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_arabica.append(data_robusta).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e133dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = map(str.lower, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426c4c2",
   "metadata": {},
   "source": [
    "So we can notice that even after ccombining the data from both the species statistical features dont change much except a few points in some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08daf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.country_of_origin == 'USA'].region.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.get_dummies(data, columns = ['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe495ff7",
   "metadata": {},
   "source": [
    "### Missing values imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79de002",
   "metadata": {},
   "source": [
    "Let's check the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3189ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1cc6f",
   "metadata": {},
   "source": [
    "For the ease of usage get the percentage of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.isnull().sum()/data.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1977f",
   "metadata": {},
   "source": [
    "We notice that there are eight columns with missing values (0.07% to 17%): three columns with continous (float) values, four categorical columns, and one is integer column. We might not need some of these columns at the end, yet first deal with the missing values. Dealing with missing values is always depends on the and the ratio of missing values itself. Normally, if a columns has missing values less than 50% then we can try to deal with that. Since, in all of the columns we have missing values less than even 20% so let's impute them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb7309",
   "metadata": {},
   "source": [
    "Well, there three main categories of techinques to deal with the missing values.\n",
    "\n",
    "   1. Univariate\n",
    "   2. Hybrid\n",
    "   3. Multivariate\n",
    "   \n",
    "In univariate, we normally impute the missing values using central tendency based metrics, for instance, Mean or Median (in case of numeric columns) and Mode (in case of categorical columns).\n",
    "\n",
    "Hybrid is a simple group based univariate imputation. Make groups (using group by) within the data and apply univariate imputation within that. \n",
    "\n",
    "In case of multivariate we can use advance or more sophisticated techniques like KNNImputer, Iterative Imputer etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e248ce2",
   "metadata": {},
   "source": [
    "Here, we will use the Hybrid approach to impute to missing values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79370f93",
   "metadata": {},
   "source": [
    "We can divide the whole data based on two main groups 'species' and 'country_of_origin'. Therefore, let's make these groups and use the missing values imputer within that group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58520aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.groupby(by=['country_of_origin','species']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51064f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf85b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = data.columns[data.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[missing_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe7591",
   "metadata": {},
   "source": [
    "Let's deal each column one-by-one and fill the missing values in three steps.\n",
    "\n",
    "- First, we will fill the missing values with mean/mode of the colum within ['country_of_origin','species'] group. \n",
    "- If we are still left with the missing values then we fill those only with ['country_of_origin'] group.\n",
    "- If we are still left then we can drop those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values_multi_columns(data, missing_columns):\n",
    "    for col in missing_columns:\n",
    "        if data[col].dtype==\"object\":\n",
    "            data[col].fillna(data.groupby(['country_of_origin','species'])[col].transform(statistics.mode), inplace = True)\n",
    "#             data[col] = data.groupby(['country_of_origin','species'])[col].transform(lambda x: x.fillna(statistics.mode(x)))\n",
    "        if data[col].dtype==\"int64\" or data[col].dtype=='float64':\n",
    "#             data[col] = data.groupby(['country_of_origin','species'])[col].transform(lambda x: x.fillna(x.mean()))                                                                      \n",
    "            data[col].fillna(data.groupby(['country_of_origin','species'])[col].transform('mean'), inplace = True)\n",
    "#         if data[col].dtype=='float':\n",
    "#             data[col] = imp.fit_transform(data[col].values.reshape(-1,1))\n",
    "    return data\n",
    "\n",
    "def fill_missing_values_single_column(data, missing_columns):\n",
    "    for col in missing_columns:\n",
    "         if data[col].dtype==\"object\":\n",
    "#             data[col].fillna(data.groupby(['country_of_origin','species'])[col].transform(statistics.mode), inplace = True)\n",
    "            data[col] = data.groupby(['country_of_origin'])[col].transform(lambda x: x.fillna(statistics.mode(x)))\n",
    "         if data[col].dtype==\"int64\" or data[col].dtype=='float64':\n",
    "            data[col] = data.groupby(['country_of_origin'])[col].transform(lambda x: x.fillna(x.mean()))                                                                      \n",
    "            #combi[col].fillna(combi[col].mode()[0], inplace=True)\n",
    "#              data[col].fillna(data.groupby(['country_of_origin','species'])[col].transform(statistics.median), inplace = True)\n",
    "#         if data[col].dtype=='float':\n",
    "#             data[col] = imp.fit_transform(data[col].values.reshape(-1,1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ac0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fill_missing_values_multi_columns(data.copy(), missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0439fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[missing_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462dfaf",
   "metadata": {},
   "source": [
    "We saw that even after imputing based 'country_of_origin' and 'species' still we are left with missing values. Let's check that scenario with couple of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed379220",
   "metadata": {},
   "source": [
    "Let's do another iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18284dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fill_missing_values_multi_columns(data.copy(), missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[missing_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd9b92",
   "metadata": {},
   "source": [
    "We notice that within that species all the values are null for region column of that country of origin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd395b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fill_missing_values_single_column(data.copy(), missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[missing_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58819f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfce77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dddbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[missing_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85286f",
   "metadata": {},
   "source": [
    "### Feature Selection and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0853d9",
   "metadata": {},
   "source": [
    "We cannot use all of the given feature columns for our final modelling. By looking at the data we can simply drop some of the columns (e.g. status, owner). Let's drop these columns first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f06eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6daaa4b",
   "metadata": {},
   "source": [
    "status value remains constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001dd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['owner'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b0996",
   "metadata": {},
   "source": [
    "With an abstract level thinking we can surely say that these columns wont be helpul for the rest of the analysis. Let's drop them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21382728",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['status','owner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf322419",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e1033",
   "metadata": {},
   "source": [
    " Lets evaluate rest of feature columns and see if we can drop that or no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ea329",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.country_of_origin == 'India'].region.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.country_of_origin == 'USA'].region.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6fe06",
   "metadata": {},
   "source": [
    "we may find plenty of the cases where region is same but written with incorrect spellings. In another case a region of India 'CHIKMAGALUR' is is given as a region 'USA'. Therefore, we cannot trust this column as well. One option could be to apply some advanced NLP-based techniques to stream line these anomalies. However, its seems we dont need this column for the high level analysis. Let's drop this one as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['color'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processing_method'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e7694",
   "metadata": {},
   "source": [
    "So, we are only left with five categorical feature columns (species, country_of_origin, variety, processing_method, color). These columns perhaps would be help to answer the questions given with the challenge and more importantly to build predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf15c78",
   "metadata": {},
   "source": [
    "Let's also check if we need time based colums like 'grading_year' or 'harvest_year'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axes = axes.ravel()\n",
    "\n",
    "sns.boxplot(x='grading_year',y='quality_score', data = data , ax = axes[0])\n",
    "sns.boxplot(x='harvest_year',y='quality_score', data = data , ax = axes[1])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d7ace",
   "metadata": {},
   "source": [
    "Since year is also a time based but a numeric featues so we can some put that in correlation matrix as well. Let's see how it would behave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005be903",
   "metadata": {},
   "source": [
    "Let's have a look at the non-categorical columns to see if they have any kind of correlation with the target colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = data.select_dtypes(include=[np.float64, np.int64])#.drop(columns = ['harvest_year','grading_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a26d35",
   "metadata": {},
   "source": [
    "Let's see we can find something from correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.heatmap(data_num.corr(), annot=True)\n",
    "plt.title('Fig-2: Correlations', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c14871c",
   "metadata": {},
   "source": [
    "<b>Inference 1: </b>We can notice columns ('quality_score', 'aroma', 'flavor', 'aftertaste', 'acidity', 'body',\n",
    "       'balance', 'clean_cup', 'sweetness', 'cupper_points') are highly correlated with the quality_score, columns (\n",
    "       'quakers',\n",
    "       'altitude_numeric', 'harvest_year','grading_year') doesnt provide much information regarding the quality_score, while columns ('moisture',\n",
    "      'category_one_defects', 'category_two_defects') are negatively correlated with our traget feature quality_score. Here, we also notice the higher the defect a coffee has lower the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26285d46",
   "metadata": {},
   "source": [
    "Since ('quakers', 'altitude_numeric') do not provide much information so let's drop them. Let's keep the negatively correlated features for a while. Later on, we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.drop(columns = ['quakers', 'altitude_numeric'], inplace = True)\n",
    "data.drop(columns = ['quakers', 'altitude_numeric'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7002775",
   "metadata": {},
   "source": [
    "We will keep the years based features just for the purpose of some analysis. We will drop them just before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d279920",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6c47a",
   "metadata": {},
   "source": [
    "### Adding Continent column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c32ce",
   "metadata": {},
   "source": [
    "last but not the least. since we need to do some analysis based on continent central_america so lets add a column to identify if the country is from central america or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['country_of_origin'] = data['country_of_origin'].apply(lambda x: x.split(' ')[0] if x.split(' ')[0] == 'USA' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_to_continent(country_name):\n",
    "    country_alpha2 = pc.country_name_to_country_alpha2(country_name)\n",
    "    country_continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "    country_continent_name = pc.convert_continent_code_to_continent_name(country_continent_code)\n",
    "    return country_continent_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.country_of_origin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "central_america = ['Mexico','Guatemala','Honduras','Nicaragua','El Salvador','Costa Rica','Panama','Belize']\n",
    "cont = ['Central America' if country in central_america else country_to_continent(country) for country in data.country_of_origin.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['continent'] = cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d8f6f",
   "metadata": {},
   "source": [
    "### Feature Scaling and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6f8c8",
   "metadata": {},
   "source": [
    "We already noticed that distributions of our numeric features are not on a single scale which might hinder the model to converge quickly. Therefore, we have to apply scaling to make to in a single range. We can apply this scaling here or add the scaling and encoding functions in the prediction pipe line. It would be more organized to keep it with the prediction pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c69d2",
   "metadata": {},
   "source": [
    "However, if we want to do it here then uncomment the script given belowSo, Let's apply simple features scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da22ee9",
   "metadata": {},
   "source": [
    "Let's first create a copy of data which we will use for modeling. The original copy will be used for EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46150a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_data = data.copy()\n",
    "\n",
    "# num_cols = list(data.select_dtypes(include=['int64','float64']))\n",
    "# num_cols.remove('quality_score')\n",
    "# num_cols.remove('grading_year')\n",
    "# num_cols.remove('harvest_year')\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# transformed_data[num_cols] = scaler.fit_transform(transformed_data[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4fca16",
   "metadata": {},
   "source": [
    "We are not interested to scale target variable or year columns. All the numeric columns exluding target column ('quality_year') and years columns need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8603d079",
   "metadata": {},
   "source": [
    "In case we want to use categorical variable then we have encode them as numeric column but it cant be pure numeric. the best way to encode them is to use label-encoder or one-hot-encoder. It would be more flexible to use that along with the prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b145eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoders = {}\n",
    "\n",
    "# transformed_data = data.copy()\n",
    "\n",
    "# cat_columns =  ['species', 'country_of_origin', 'variety', 'processing_method', 'color','altitude_uom']\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# transformed_data[cat_columns] = le.fit_transform(transformed_data[cat_columns])\n",
    "\n",
    "# for col in cat_columns:\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "    \n",
    "#     transformed_data[col] = le.fit_transform(data[col])\n",
    "    \n",
    "#     encoders[col] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba6c89",
   "metadata": {},
   "source": [
    "Our transformed data is ready for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54784370",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dda413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744e33b",
   "metadata": {},
   "source": [
    "We are interested to see formation from the very high level like continent. So, let's start with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "((data['continent'].value_counts()/len(data['continent']))*100).sort_values(ascending = False).plot.pie()#.plot(kind = 'bar')\n",
    "#plt.ylabel('Percentage')\n",
    "#plt.xlabel('Continents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e127f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "((data['continent'].value_counts()/len(data['continent']))*100).sort_values(ascending = False).plot(kind = 'bar')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Continents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b78c65",
   "metadata": {},
   "source": [
    "We have 45%  of the cases are relevant to the contries in central americal. In other words we can say that these countries have produceed about 45% of the whole production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e09e6",
   "metadata": {},
   "source": [
    "Let's the distribution of quality_score wrt to each continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59611f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.groupby('continent')['quality_score'].median().reset_index().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"continent\", y=\"quality_score\", data=data)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeda54c",
   "metadata": {},
   "source": [
    "As per the distribution, we notice that Central America has a wider dsitribution with some outliers. Moreover, it has lower median value than the quality scores from other continents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ff814",
   "metadata": {},
   "source": [
    "We need to analyze on the core factors behind the quality score. In Fig-1, we already saw that quality_score is highly correlated with ('aroma', 'flavor', 'aftertaste', 'acidity', 'body', 'balance', 'clean_cup', 'sweetness', 'cupper_points') while negatively correlated with columns ('moisture', 'category_one_defects', 'category_two_defects'). So let's visualize and see the the continent wise perfromance wrt to each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf03fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 3, figsize=(15, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "cols = ['aroma', 'flavor', 'aftertaste',\n",
    "       'acidity', 'body', 'balance', 'clean_cup', 'sweetness', 'cupper_points',\n",
    "       'moisture', 'category_one_defects', 'category_two_defects']\n",
    "\n",
    "\n",
    "\n",
    "for axs, col in zip(axes, cols):\n",
    "    plot = sns.boxplot(x='continent', y=col, data=data, ax = axs)\n",
    "    _ = plot.set(title='({})'.format(col))\n",
    "    axs.tick_params(labelrotation=45)\n",
    "    plt.xticks(rotation = 45)\n",
    "axes[-3].set_visible(False)\n",
    "axes[-2].set_visible(False)\n",
    "axes[-1].set_visible(False)\n",
    "#fig.title('month-wise sales distribution of each season', fontsize=16)\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_data_agg = data.groupby(['continent'])[['quality_score', 'aroma', 'flavor', 'aftertaste', 'acidity', 'body',\n",
    "       'balance', 'clean_cup', 'sweetness', 'cupper_points', 'moisture',\n",
    "       'category_one_defects', 'category_two_defects']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_data_agg.sort_values(by='quality_score', ascending = False).style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0515b",
   "metadata": {},
   "source": [
    "We notice that mostly in case of highly correlated features, 'Central America' is performing compartively poor than other continents and better on the negatively correlated features especially category_two_defect and moisture. Moreover, another we notice that the distribtion of values for 'Central America' in each feature is quite wide along with having many outliers. Which indicated a sort of inconsistency in production. This inconistency may be due to the variation from different countries in this region. We will see that later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744b384",
   "metadata": {},
   "source": [
    "Potentially, there could be three factors behind this: 1) variety, 2) any specific country within region with poor performance, or 3) processing_method. Let's investigate both of these aspects and compare them with rest of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ca43e",
   "metadata": {},
   "source": [
    "#### Varieties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ecc00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_data = data[data.continent == 'Central America'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd51ce",
   "metadata": {},
   "source": [
    "First let's invetigate the quality of all varieties the framers in Central America producing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ead83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_variety_agg = CA_data.groupby('variety')['quality_score'].count().reset_index()#\n",
    "CA_variety_agg.rename(columns = {'quality_score':'count'}, inplace = True)\n",
    "CA_variety_agg.sort_values(by = 'count', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54292c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# axes = axes.ravel()\n",
    "\n",
    "sns.barplot(x='variety', y='count', data=CA_variety_agg, palette = sns.color_palette('tab10'))\n",
    "\n",
    "# sns.barplot(x='variety', y='mean', data=CA_variety_agg, ax=axes[1], palette = sns.color_palette('tab10'))\n",
    "# axes[1].tick_params(labelrotation=45)\n",
    "#fig.tight_layout()\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_variety_agg_all = CA_data.groupby('variety')[['quality_score', 'aroma', 'flavor', 'aftertaste', 'acidity', 'body',\n",
    "       'balance', 'clean_cup', 'sweetness', 'cupper_points', 'moisture',\n",
    "       'category_one_defects', 'category_two_defects']].mean().reset_index()\n",
    "CA_variety_agg = CA_variety_agg.merge(CA_variety_agg_all, how = 'inner', on = 'variety').sort_values(by = ['count','quality_score'], ascending = False)\n",
    "CA_variety_agg.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94e94a",
   "metadata": {},
   "source": [
    "The major chunk of production comprises on three varieties (Bourban,Typica, and Caturra). We can notice that 'Typica' holding the second highest chunk has inferior score but highest defect score as compared to the third one 'Catura' and infact all the varieties in top 5. So, first suggestion we can make is to drop Typica or get it least production. \n",
    "\n",
    "<b> Inference-2: </b> Drop 'Typica' or at least decrease its production as compared to the 'Catura'. 'Catura' can give them better score with less defect score. Moreover, we have some examples from the coffies produced in small quantities but people liked them very much. May take look at them if it would worthfull to promote them more e.g. 'Gesha'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cced97",
   "metadata": {},
   "source": [
    "Let's see if we can get something by comparing our inference with rest of regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c387a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variety_agg = data.groupby('variety')['quality_score'].count().reset_index()#\n",
    "data_variety_agg.rename(columns = {'quality_score':'count'}, inplace = True)\n",
    "data_variety_agg.sort_values(by = 'count', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13563754",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variety_agg_all = data.groupby('variety')[['quality_score', 'aroma', 'flavor', 'aftertaste', 'acidity', 'body',\n",
    "       'balance', 'clean_cup', 'sweetness', 'cupper_points', 'moisture',\n",
    "       'category_one_defects', 'category_two_defects']].mean().reset_index()\n",
    "data_variety_agg = data_variety_agg.merge(data_variety_agg_all, how = 'inner', on = 'variety').sort_values(by = ['count','quality_score'], ascending = False)\n",
    "data_variety_agg.head(15).style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_variety_agg = data.groupby('variety')['quality_score','flavor','aftertaste','balance','cupper_points'].mean().reset_index().sort_values(by = ['quality_score','flavor','aftertaste','balance','cupper_points'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e765fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_variety_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variety_agg = data.groupby('variety')['quality_score'].agg(['mean','count']).reset_index().sort_values(by = 'count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='variety', y='count', data=data_variety_agg, palette = sns.color_palette('tab10'))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31bbc4",
   "metadata": {},
   "source": [
    "By looking at the other regions we can verify or Inference-2: Increasing the production of 'Catura' and decreasing the 'Typica'. Moreover, try new varaities like 'Gesha' or 'Yellow Bourbon'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28fe53",
   "metadata": {},
   "source": [
    "##### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_country_agg = CA_data.groupby('country_of_origin')['quality_score'].count().reset_index()#\n",
    "CA_country_agg.rename(columns = {'quality_score':'count'}, inplace = True)\n",
    "CA_country_agg.sort_values(by = 'count', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e898be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# axes = axes.ravel()\n",
    "\n",
    "sns.barplot(x='country_of_origin', y='count', data=CA_country_agg, palette = sns.color_palette('tab10'))\n",
    "\n",
    "# sns.barplot(x='variety', y='mean', data=CA_variety_agg, ax=axes[1], palette = sns.color_palette('tab10'))\n",
    "# axes[1].tick_params(labelrotation=45)\n",
    "#fig.tight_layout()\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_country_agg_all = CA_data.groupby('country_of_origin')[['quality_score', 'aroma', 'flavor', 'aftertaste', 'acidity', 'body',\n",
    "       'balance', 'clean_cup', 'sweetness', 'cupper_points', 'moisture',\n",
    "       'category_one_defects', 'category_two_defects']].mean().reset_index()\n",
    "CA_country_agg = CA_country_agg.merge(CA_country_agg_all, how = 'inner', on = 'country_of_origin').sort_values(by = ['count','quality_score'], ascending = False)\n",
    "CA_country_agg.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7bd0c0",
   "metadata": {},
   "source": [
    "We see that about 72% of the production comes from Mexico (41%) and Guatemala (31%). However, Mexico has slighty lower score than Guatemala but more importanly very high score for negative correllated features like moisture, category_one_defect, and category_two_defect. The Farmers in Mexico should more focus on to reduce the moisture and reduce the defects. This will ultimately get them better quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mexico_variety = CA_data[CA_data.country_of_origin == 'Mexico']['variety'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='index',y='variety', data = Mexico_variety)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433f08b",
   "metadata": {},
   "source": [
    "The main reason behind the Mexico farmers getting lower scores is the variety 'Typica'. We already saw that 'Typica' has more defects and higher moisture level than most of the other which makes it bit less attractive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be941d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(5, 3, figsize=(15, 10))\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# cols = ['quality_score', 'aroma', 'flavor', 'aftertaste',\n",
    "#        'acidity', 'body', 'balance', 'clean_cup', 'sweetness', 'cupper_points',\n",
    "#        'moisture', 'category_one_defects', 'category_two_defects']\n",
    "\n",
    "\n",
    "\n",
    "# for axs, col in zip(axes, cols):\n",
    "#     plot = sns.barplot(x='continent', y=col, data=cont_data_agg, ax = axs)\n",
    "#     _ = plot.set(title='({})'.format(col))\n",
    "# axes[-2].set_visible(False)\n",
    "# axes[-1].set_visible(False)\n",
    "# #fig.title('month-wise sales distribution of each season', fontsize=16)\n",
    "# fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_data[CA_data.country_of_origin == 'Mexico']['processing_method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_data[CA_data.country_of_origin == 'Guatemala']['processing_method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632473bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.continent == 'Central America']['processing_method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767034f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processing_method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d64443",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.continent == 'Central America']['variety'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18452e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.continent == 'Africa']['variety'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954a973",
   "metadata": {},
   "source": [
    "Let's transform these features to find the relationship with out dependent feature (quality_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356ff0e",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497bc4c9",
   "metadata": {},
   "source": [
    "Since records for both species are clustered togther so let's shuffle the data and seperate the data and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26276aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "\n",
    "cols = ['aroma', 'flavor', 'aftertaste',\n",
    "       'acidity', 'body', 'balance', 'clean_cup', 'sweetness', 'cupper_points',\n",
    "       'moisture', 'category_one_defects', 'category_two_defects']\n",
    "\n",
    "\n",
    "for axs, col in zip(axes, cols):\n",
    "    plot = sns.regplot(x='quality_score', y=col, data=data, ax = axs)\n",
    "    _ = plot.set(title='({})'.format(col))\n",
    "axes[-3].set_visible(False)\n",
    "axes[-2].set_visible(False)\n",
    "axes[-1].set_visible(False)\n",
    "fig.suptitle('Fig-3: Linear Relationship')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac64d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['quality_score'])\n",
    "Y = data['quality_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8a3ec",
   "metadata": {},
   "source": [
    "### Feature scaling and encoding processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c05366",
   "metadata": {},
   "source": [
    "Instead doing features scaling in the preprocessing phase, we opted to keep it more simple and put that along the prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = numerical_columns_selector(X)\n",
    "categorical_columns = categorical_columns_selector(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffe533",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns.remove('grading_year')\n",
    "numerical_columns.remove('harvest_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns.remove('altitude_uom')\n",
    "categorical_columns.remove('continent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5831a7",
   "metadata": {},
   "source": [
    "We would like to check what would be the performance of model by including or exlcluding the important categorical features. Therefore, first we define the pipeline with categorical features and then later on we will train the model only uisng numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('minmax_scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca70c1d",
   "metadata": {},
   "source": [
    "### Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = make_pipeline(comb_preprocessor, LinearRegression())\n",
    "tree_model = make_pipeline(comb_preprocessor, DecisionTreeRegressor())\n",
    "forest_model = make_pipeline(comb_preprocessor, RandomForestRegressor(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024632c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = transformed_data.drop(columns=['quality_score','continent','harvest_year','grading_year','species', 'country_of_origin', 'variety', 'processing_method', 'color','altitude_uom'])\n",
    "# Y = transformed_data['quality_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0415c00",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(linear_model, X, Y, cv=5, scoring=['r2','neg_mean_squared_error'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(tree_model, X, Y, cv=5, scoring=['r2','neg_mean_squared_error'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48582fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(forest_model, X, Y, cv=5, scoring=['r2','neg_root_mean_squared_error'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165dd1f",
   "metadata": {},
   "source": [
    "<b> Only numeric features: </b> Now let's use only numeric features (identified using the Fig-2) to fit our model and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([('minmax_scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ebba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = make_pipeline(preprocessor, LinearRegression())\n",
    "tree_model = make_pipeline(comb_preprocessor, DecisionTreeRegressor())\n",
    "forest_model = make_pipeline(preprocessor, RandomForestRegressor(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c40eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(linear_model, X[numerical_columns], Y, cv=5, scoring=['r2','neg_root_mean_squared_error'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57063e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(forest_model, X[numerical_columns], Y, cv=5, scoring=['r2','neg_root_mean_squared_error'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2f9e8",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62752dbc",
   "metadata": {},
   "source": [
    "Finally, we saw the linear regression (with both categorical and numerical features) has performed best among all the regression based model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7e9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "# ...     X, Y, test_size=0.20, random_state=42)\n",
    "# lr = LinearRegression().fit(X_train[numerical_columns], y_train)\n",
    "# lr.score(X_train[numerical_columns], y_train)\n",
    "# numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9486d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfb4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
